---
title: "576 Final Project Script"
output: html_document
authors: Alexander Baker, Olivia Ward
date: "2022-11-18"
---

#Library Archive
```{r}
library(h2o)
h2o.startLogging()
```

#Get h2o help
```{r}
?h2o
```

#Reset h2o
```{r}
h2o.shutdown
```

#Step 1: Read data sets train and test and view them
```{r}
h2o.init()
train <- h2o.importFile(path = "train.csv")
test <- h2o.importFile(path = "test.csv")
train
test
```

#Step 2: Take our train data, and data mine it with different methods.
Method 1 SUB3: Deep Learning
```{r}
y <- "SalePrice"
x <- setdiff(names(train), y)

  dl <- h2o.deeplearning(
      y="SalePrice",
      training_frame=train,
      model_id = NULL,
      validation_frame = NULL,
      nfolds = 0,
      keep_cross_validation_models = TRUE,
      keep_cross_validation_predictions = FALSE,
      keep_cross_validation_fold_assignment = FALSE,
      fold_assignment = "AUTO",
      fold_column = NULL,
      ignore_const_cols = TRUE,
      score_each_iteration = FALSE,
      weights_column = NULL,
      offset_column = NULL,
      balance_classes = FALSE,
      class_sampling_factors = NULL,
      max_after_balance_size = 5,
      checkpoint = NULL,
      pretrained_autoencoder = NULL,
      overwrite_with_best_model = TRUE,
      use_all_factor_levels = TRUE,
      standardize = TRUE,
      activation = "Rectifier",
      hidden = c(200, 200),
      epochs = 10,
      train_samples_per_iteration = -2,
      target_ratio_comm_to_comp = 0.05,
      seed = -1,
      adaptive_rate = TRUE,
      rho = 0.99,
      epsilon = 1e-08,
      rate = 0.005,
      rate_annealing = 1e-06,
      rate_decay = 1,
      momentum_start = 0,
      momentum_ramp = 1e+06,
      momentum_stable = 0,
      nesterov_accelerated_gradient = TRUE,
      input_dropout_ratio = 0,
      hidden_dropout_ratios = NULL,
      l1 = 0,
      l2 = 0,
      max_w2 = 3.4028235e+38,
      initial_weight_distribution = "UniformAdaptive",
      initial_weight_scale = 1,
      initial_weights = NULL,
      initial_biases = NULL,
      loss = "Automatic", 
      distribution = "AUTO",
      quantile_alpha = 0.5,
      tweedie_power = 1.5,
      huber_alpha = 0.9,
      score_interval = 5,
      score_training_samples = 10000,
      score_validation_samples = 0,
      score_duty_cycle = 0.1,
      classification_stop = 0,
      regression_stop = 1e-06,
      stopping_rounds = 5,
      stopping_metric = "AUTO",
      stopping_tolerance = 0,
      max_runtime_secs = 0,
      score_validation_sampling = "Uniform",
      diagnostics = TRUE,
      fast_mode = TRUE,
      force_load_balance = TRUE,
      variable_importances = TRUE,
      replicate_training_data = TRUE,
      single_node_mode = FALSE,
      shuffle_training_data = FALSE,
      missing_values_handling = "MeanImputation",
      quiet_mode = FALSE,
      autoencoder = FALSE,
      sparse = FALSE,
      col_major = FALSE,
      average_activation = 0,
      sparsity_beta = 0,
      max_categorical_features = 2147483647,
      reproducible = FALSE,
      export_weights_and_biases = FALSE,
      mini_batch_size = 1,
      categorical_encoding = "AUTO",
      elastic_averaging = FALSE,
      elastic_averaging_moving_rate = 0.9,
      elastic_averaging_regularization = 0.001,
      export_checkpoints_dir = NULL,
      auc_type = "AUTO",
      verbose = FALSE
      )
```
Method 2 SUB4: Gradient Boosting
```{r}
y <- "SalePrice"
x <- setdiff(names(train), y)

  gb <-  h2o.xgboost(
      x,
      y,
      training_frame=train,
      model_id = NULL,
      validation_frame = NULL,
      nfolds = 0,
      keep_cross_validation_models = TRUE,
      keep_cross_validation_predictions = FALSE,
      keep_cross_validation_fold_assignment = FALSE,
      score_each_iteration = FALSE,
      fold_assignment = "AUTO",
      fold_column = NULL,
      ignore_const_cols = TRUE,
      offset_column = NULL,
      weights_column = NULL,
      stopping_rounds = 0,
      stopping_metric = "AUTO",
      stopping_tolerance = 0.001,
      max_runtime_secs = 0,
      seed = -1,
      distribution = "AUTO",
      tweedie_power = 1.5,
      categorical_encoding = "AUTO",
      quiet_mode = TRUE,
      checkpoint = NULL,
      export_checkpoints_dir = NULL,
      ntrees = 50,
      max_depth = 6,
      min_rows = 1,
      min_child_weight = 1,
      learn_rate = 0.3,
      eta = 0.3,
      sample_rate = 1,
      subsample = 1,
      col_sample_rate = 1,
      colsample_bylevel = 1,
      col_sample_rate_per_tree = 1,
      colsample_bytree = 1,
      colsample_bynode = 1,
      max_abs_leafnode_pred = 0,
      max_delta_step = 0,
      monotone_constraints = NULL,
      interaction_constraints = NULL,
      score_tree_interval = 0,
      min_split_improvement = 0,
      gamma = 0,
      nthread = -1,
      save_matrix_directory = NULL,
      build_tree_one_node = FALSE,
      calibrate_model = FALSE,
      calibration_frame = NULL,
      calibration_method = "AUTO",
      max_bins = 256,
      max_leaves = 0,
      sample_type = "uniform",
      normalize_type = "tree",
      rate_drop = 0,
      one_drop = FALSE,
      skip_drop = 0,
      tree_method = "auto",
      grow_policy = "depthwise",
      booster = "gbtree",
      reg_lambda = 1,
      reg_alpha = 0,
      dmatrix_type = "auto",
      backend = "auto",
      gpu_id = NULL,
      gainslift_bins = -1,
      auc_type = "AUTO",
      scale_pos_weight = 1,
      verbose = FALSE
      )
```
Method 3: AutoML NOT WORKING
DELETED.
Method 4 SUB1: Decision Tree
```{r}
y <- "SalePrice"
x <- setdiff(names(train), y)

rf <- h2o.randomForest(
      x,
      y,
      training_frame = train,
      model_id = NULL,
      validation_frame = NULL,
      nfolds = 0,
      keep_cross_validation_predictions = FALSE,
      keep_cross_validation_fold_assignment = FALSE,
      score_each_iteration = FALSE,
      score_tree_interval = 0,
      fold_column = NULL,
      ignore_const_cols = TRUE,
      offset_column = NULL,
      weights_column = NULL,
      balance_classes = FALSE,
      class_sampling_factors = NULL,
      max_after_balance_size = 5,
      ntrees = 50,
      max_depth = 20,
      min_rows = 1,
      nbins = 20,
      nbins_top_level = 1024,
      nbins_cats = 1024,
      r2_stopping = Inf,
      stopping_rounds = 0,
      stopping_tolerance = 0.001,
      max_runtime_secs = 0,
      seed = -1,
      build_tree_one_node = FALSE,
      mtries = -1,
      sample_rate = 0.632,
      sample_rate_per_class = NULL,
      binomial_double_trees = FALSE,
      checkpoint = NULL,
      col_sample_rate_change_per_level = 1,
      col_sample_rate_per_tree = 1,
      min_split_improvement = 1e-05,
      calibrate_model = FALSE,
      calibration_frame = NULL,
      verbose = FALSE
    )
```

Method 5 SUB5: TUNED Gradient boosted classification Tree
```{r}
y <- "SalePrice"
x <- setdiff(names(train), y)

gbct <- h2o.gbm( 
      x,
      y,
      training_frame = train,
      model_id = NULL,
      validation_frame = NULL,
      nfolds = 4,
      keep_cross_validation_models = TRUE,
      keep_cross_validation_predictions = FALSE,
      keep_cross_validation_fold_assignment = FALSE,
      score_each_iteration = FALSE,
      score_tree_interval = 0,
      fold_assignment = "AUTO",
      fold_column = NULL,
      ignore_const_cols = FALSE,
      offset_column = NULL,
      weights_column = NULL,
      balance_classes = FALSE,
      class_sampling_factors = NULL,
      max_after_balance_size = 5,
      ntrees = 180,
      max_depth = 8,
      min_rows = 5,
      nbins = 20,
      nbins_top_level = 1024,
      nbins_cats = 1024,
      stopping_rounds = 0,
stopping_metric = "AUTO",
      stopping_tolerance = 0.001,
      max_runtime_secs = 0,
      seed = -1,
      build_tree_one_node = FALSE,
      learn_rate = 0.1,
      learn_rate_annealing = 1,
distribution = "AUTO",
      quantile_alpha = 0.5,
      tweedie_power = 1.5,
      huber_alpha = 0.9,
      checkpoint = NULL,
      sample_rate = 1,
      sample_rate_per_class = NULL,
      col_sample_rate = 1,
      col_sample_rate_change_per_level = 1,
      col_sample_rate_per_tree = 1,
      min_split_improvement = 1e-05,
      histogram_type = "AUTO",
      max_abs_leafnode_pred = Inf,
      pred_noise_bandwidth = 0,
      categorical_encoding = "AUTO",
      calibrate_model = FALSE,
      calibration_frame = NULL,
      calibration_method = "AUTO",
      custom_metric_func = NULL,
      custom_distribution_func = NULL,
      export_checkpoints_dir = NULL,
      in_training_checkpoints_dir = NULL,
      in_training_checkpoints_tree_interval = 1,
      monotone_constraints = NULL,
      check_constant_response = TRUE,
      gainslift_bins = -1,
      auc_type = "AUTO",
      interaction_constraints = NULL,
      verbose = FALSE
    )
```

#Step 3: Find which model is the best by checking its performance.
```{r}
h2o.performance(gbct)
```
#Step 3 Archive
Key - MSE, RMSE, MAE, RMSLE, MRD: lower is more accurate

#Method 4 SUB1: Decision Tree

H2ORegressionMetrics: drf
** Reported on training data. **
** Metrics reported on Out-Of-Bag training samples **

MSE:  961187613
RMSE:  31003.03
MAE:  17864.01
RMSLE:  0.1422684
Mean Residual Deviance :  961187613

#Method 5 SUB2: Gradient boosted classification Tree

H2ORegressionMetrics: gbm
** Reported on training data. **

MSE:  196409342
RMSE:  14014.61
MAE:  8504.974
RMSLE:  0.07432775
Mean Residual Deviance :  196409342

#Method 1 SUB3: Deep Learning

H2ORegressionMetrics: deeplearning
** Reported on training data. **
** Metrics reported on full training frame **

MSE:  240355105
RMSE:  15503.39
MAE:  11284.47
RMSLE:  0.08966187
Mean Residual Deviance :  240355105

#Method 2 SUB4: Gradient Boosting

H2ORegressionMetrics: xgboost
** Reported on training data. **

MSE:  12188431
RMSE:  3491.193
MAE:  2562.853
RMSLE:  0.02362581
Mean Residual Deviance :  12188431

#Method SUB5: tuned GBCT V2
H2ORegressionMetrics: gbm
** Reported on training data. **

MSE:  33153786
RMSE:  5757.932
MAE:  3589.733
RMSLE:  0.03345172
Mean Residual Deviance :  33153786

#Method SUB6: tuned GBCT V3

H2ORegressionMetrics: gbm
** Reported on training data. **

MSE:  4805144
RMSE:  2192.064
MAE:  1434.337
RMSLE:  0.01430637
Mean Residual Deviance :  4805144

#Step 4: Take winning model and process the data to give a submission dataframe.
```{r}
model <- h2o.predict(gbct,test)
```


#Step 5: Write dataframe to .csv for final submission
```{r}
h2o.exportFile(model,"submission6.csv")
```

